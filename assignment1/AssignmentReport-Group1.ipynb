{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "$$C^n = -(y^n * ln(\\hat{y}^n) + (1-y^n)*ln(1-\\hat{y}^n))$$\n",
    "$$\\frac{\\partial C^n}{\\partial w} = -(y^n * \\frac{(\\hat{y}^n)'}{\\hat{y}^n} + (1-y^n) * \\frac{(1-\\hat{y}^n)'}{1-\\hat{y}^n})$$\n",
    "\n",
    "$$\\hat{y}^n = f(x) = \\frac{1}{1+e^{-w^T x}}$$\n",
    "$$(\\hat{y}^n)' = f'(x) = - \\frac{e^{-w^T x}*x}{(1+e^{-w^T x})^2}$$\n",
    "$$f'(x) = \\frac{1}{1+e^{-w^T x}} * \\frac{e^{-w^T x}}{1+e^{-w^T x}} * x$$\n",
    "\n",
    "We recognize that the first fraction is the sigmoid function, and the second fraction can also be simplified to the sigmoid function by adding and subtracting by 1\n",
    "\n",
    "$$f'(x) = f(x) * (1-f(x)) * x$$\n",
    "\n",
    "We now use this in our original equation:\n",
    "\n",
    "$$\\frac{\\partial C^n}{\\partial w} = -(y^n * \\frac{\\hat{y}^n * (1-\\hat{y}^n)*x}{\\hat{y}^n} + (1-y^n) * \\frac{-\\hat{y}^n * (1-\\hat{y}^n)*x}{1-\\hat{y}^n})$$\n",
    "\n",
    "$$\\frac{\\partial C^n}{\\partial w} = -(y^n x - y^n \\hat{y}^n x - \\hat{y}^n x +y^n \\hat{y}^n x)$$\n",
    "$$\\frac{\\partial C^n}{\\partial w} = -(y^n - \\hat{y}^n)*x$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stopping kicks in after only 33 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "The spikes in the accuracy when data is not shuffled is possibly due to the fact that a certain segment of the training data is not representative of the validation dataset. Shuffling the order of training data increases generality, and therefore the oscillations in accuracy are reduced.\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Yes, we can notice signs of overfitting based on the differing training accuracies. The gap between the training and validation is increasing towards the end of training, which is a sign of overfitting. However, the validation accuracy is still slightly increasing, so it is not clear-cut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "We have already found the update term for $\\frac{\\partial C(w)}{\\partial w}$, so we only need to find out what the update term is for regularization. This will simply be $\\lambda W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The bottom row is less noisy due to the difference in weight values. The regularized weights have smaller differences in values, and therefore appear to have more smooth transitions visually. Regularization also forces the model to not overfit on noise, keeping the model more generalizable.\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "It seems that the model is underfitting when using lambdas between 1 and 0.1, but I don't fully understand why the other models also perform worse. Perhaps the distribution between training and validation set is good enough that regularization has no meaningful impact, and simply worsens the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "We can observe that the models with lower lambdas for the l2 regularization clearly have a higher l2 norm, which makes sense given that regularization is meant to punish large weight values.\n",
    "\n",
    "![](task4e_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
