{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{ji}} = \\frac{\\partial z_j}{\\partial w_{ji}} * \\frac{\\partial C }{\\partial z_j}$$\n",
    "$$ \\frac{\\partial z_j}{\\partial w_{ji}} = \\frac{w_{ji}*x_i}{\\partial w_{ji}} = x_i  $$\n",
    "$$ w_{ji} = w_{ji} - \\alpha * {\\delta}_j * x_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Since the bias is implicitly included in the weights, the amount of parameters can be calculated by finding the amount of weights. This is 785 \\* 64 + 64 \\* 10 = 50 880."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3a)\n",
    "![](task3a_train_loss.png)\n",
    "\n",
    "We can see that initializing weights using a normal distribution led to a better accuracies for training and validation set at the end, as well as giving a much better starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3b)\n",
    "![](task3b_train_loss.png)\n",
    "\n",
    "Using the improved sigmoid function, we can see that the network converges even faster. The reduction in loss is steeper than the previous graph, and training stops early because early stopping kicks in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3c)\n",
    "![](task3c_train_loss.png)\n",
    "\n",
    "No clear improvement when including momentum from what I can see. Seems to be increase between training and validation loss in previous graph, while the current graph has a smaller distance throughout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "![](task4a_train_loss.png)\n",
    "\n",
    "We can observe that the validation loss and accuracy is worse than previously, and that early stopping kicked in for this run. Training accuracy has not yet converged as it did previously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "![](task4b_train_loss.png)\n",
    "\n",
    "The learning is less \"smooth\" in the beginning of the learning phase compared to the 3c example. Note that this is however the highest accuracy achieved thus far in the validation set, so I would personally say this is the best network so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_train_loss.png)\n",
    "We have not changed any of the parameters of the original network in task 3, only applied different tricks. The amount of parameters is therefore still 50 880. The new network has 45 920 parameters with a topology of: 32,128,32,10. This network performed slightly worse than the previous network with a single hidden layer. This might be because the amount of weights from the input layer into the hidden layers are halved compared to initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4e_train_loss.png)\n",
    "\n",
    "The main difference between this network and the previous one is the variance in the loss and accuracy. Given that the amount of parameters is higher in this network compared to the previous one's, the variance could be due to that. There could also be a problem of vanishing or exploding gradients in a deep neural network. It would also probably be beneficial to include the use of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
