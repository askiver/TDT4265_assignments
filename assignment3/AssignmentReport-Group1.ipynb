{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Just need to flip the kernel and do the necessary calculations for each field:\n",
    "\n",
    "1.1 = $1*(-2) + 9*(-1) = -11$\n",
    "1.2 = $2*2 + 2*(-2) + 1*(-2) + 3*2 = 2$\n",
    "1.3 = $1*2 + 3*(-2) + 9*1 + 1*(-1) = 4$\n",
    "1.4 = $2*2 + 1*(-2) + 1*1 + 4*(-1) = -1$\n",
    "1.5 = $3*2 + 1*1 = 7$\n",
    "\n",
    "2.1 = $9*(-2) + 1*(-1) + 5*(-1) = -24$\n",
    "2.2 = $2*1 + 3*2 + 4*1 + 2*(-1) + 1*(-2) = 8$\n",
    "2.3 = $1*1 + 9*2 + 5*1 + 3*(-1) + 1*(-2) + 7*(-1) = 12$\n",
    "2.4 = $2*1 + 1*2 + 1*(-1) + 4*(-2) = -5$\n",
    "2.5 = $3*1 + 1*2 + 7*1 = 12$\n",
    "\n",
    "3.1 = $9*(-1) + 5*(-2) = -19$\n",
    "3.2 = $3*1 + 4*2 + 1*(-1) = 10$\n",
    "3.3 = $9*1 + 5*2 + 1*(-1) + 7*(-2) = 4$\n",
    "3.4 = $1*1 + 4*(-1) = -3$\n",
    "3.5 = $1*1 + 7*2 = 15$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Max Pooling layers makes the CNN's less sensitive to translational variation, as the pooling layer reduces the spatial dimensions for the next layer.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "If you want the same output shape for an image using a kernel of size $F*F$ and a stride of 1, the necessary amount of padding is $\\frac{F-1}{2}$ which is 3 in this case. So 3 in padding on all sides.\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "A kernel size of 5x5 is required to lose 4 in both height and width using those parameters.\n",
    "## task 1e)\n",
    "\n",
    "After the subsampling the dimensions of the feature maps are 254\\*254, as they are effectively .\n",
    "## task 1f)\n",
    "\n",
    "The feature maps of the second layer will have a size of 252\\*252 because of the kernel of size 3\\*3.\n",
    "## task 1g)\n",
    "Because of the filter size and padding, the convolution itself will not lead to change in output size. The MaxPool layers will halve both height and width of output.\n",
    "\n",
    "Layer 1: 5x5x3x32 + 32 = 2432 parameters. Image was originally 32x32, but reduced to 16x16 by MaxPool.\n",
    "\n",
    "Layer 2: 5x5x32x64 + 64 = 51264 parameters. Image reduced to 8x8.\n",
    "\n",
    "Layer 3: 5x5x64x128 + 128 = 204928 parameters. Image reduced to 4x4.\n",
    "\n",
    "Flatten transforms the feature maps to a 1d vector of size 4x4x128 = 2048\n",
    "\n",
    "Layer 4 = 2048x64 + 64 = 131136 parameters.\n",
    "\n",
    "Layer 5 = 64x10 + 10 = 650 parameters.\n",
    "\n",
    "In total the network has 390410 parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "\n",
    "![](plots/task2_plot.png)\n",
    "\n",
    "### Task 2b)\n",
    "\n",
    "Final train accuracy: 87.29996443812233\n",
    "Final validation accuracy: 73.48\n",
    "Final test accuracy: 73.0\n",
    "\n",
    "(Does seem like these values change somewhat from run to run, seed not set correctly?)\n",
    "\n",
    "These values make sense, as train is slightly higher than validation and test accuracy, while validation and test is very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Optimizer used was Adam. Learning rate was set to 0.0001. Batch size was 64. Weight initialization was unchanged from before, so still using Kaiming uniform I suppose. A pipeline was used for data augmentation, composed of random horizontal flip, random crop, and random color jitter. Regularization was implemented in weight_decay of Adam optimizer, set to 0.001.\n",
    "\n",
    "Network architecture:\n",
    "\n",
    "Layer 1: \n",
    "conv2d(filters=128, kernel_size=3, padding=1, stride=1, padding_mode=zeros, bias=False)\n",
    "BatchNorm2d(128)\n",
    "GELU()\n",
    "MaxPool2D(kernel_size=2, stride=2)\n",
    "\n",
    "This first layer was repeated 4 times, with only the amount of filters changing. Each layer had double the amount of layers compared to the previous. Layer 2 had 256 filters. Layer 3 had 512. Layer 4 had 1024.\n",
    "\n",
    "Layer 5:\n",
    "Linear(16384, 256, bias=False)\n",
    "BatchNorm1d(256)\n",
    "GELU()\n",
    "\n",
    "Layer 6:\n",
    "Linear(256, 64, bias=False)\n",
    "BatchNorm1d(64)\n",
    "GELU()\n",
    "\n",
    "Layer 7:\n",
    "Linear(64, 10)\n",
    "SoftMax()\n",
    "\n",
    "### Task 3b)\n",
    "Final train loss: 0.426893\n",
    "\n",
    "Final validation loss: 0.522778\n",
    "\n",
    "Final test loss: 0.525908\n",
    "\n",
    "\n",
    "\n",
    "![](plots/task3_plot.png)\n",
    "\n",
    "### Task 3c)\n",
    "\n",
    "I think the most useful method for increase in model accuracy for test set was using data augmentation. Adding noise of different varieties to the pictures helped increase generality of the model, and makes the validation loss and training loss correlate well during training.\n",
    "\n",
    "Changing activation function to GELU instead of RelU increased the accuracy somewhat, at the cost of higher training time.\n",
    "\n",
    "Changing optimizer to Adam also helped a lot, but did need to do some trial and error to find a fitting learning rate.\n",
    "\n",
    "Increasing the amount of filters also helped with accuracy, but increasing it too much led to worse performance.\n",
    "\n",
    "Batch normalization also worked wonders, at the very least for quickly finding a decent accuracy.\n",
    "\n",
    "What did not really work was changing the filter size for the convolutional layers. Both decreasing to 3, and increasing to higher values did not yield any benefit that I noticed, and only made it worse when increased by a lot. This could potentially be because the images are of quite low resolution (32x32), so large filter sizes aren't really needed to capture any specific details in the image.\n",
    "\n",
    "I also tried to remedy earlier overfitting using the weight decay of Adam optimizer. I was unable to fix this, and adjusting the value only seemed to make the network overall perform worse.\n",
    "\n",
    "### Task 3d)\n",
    "![](plots/task3d_plot.png)\n",
    "### Task 3e)\n",
    "I reached 80% accuracy before starting with task 3b, so the model there is my best model.\n",
    "\n",
    "Final test accuracy was 81.88\n",
    "\n",
    "### Task 3f)\n",
    "\n",
    "For the best model I am unable to see any specific signs of underfitting or overfitting. The loss of the validation set follows the train loss quite nicely, and the network seems to gradually perform better over time. And given that the performance of the network is adequate, I would not say it has problems with underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "FILL IN ANSWER. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
